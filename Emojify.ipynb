{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emojify- AI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow\n",
    "np.random.seed(0)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the word vectors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll be using GloVe vectors for our word embeddings. The GloVe vectors were introduced in the following paper:\n",
    "\n",
    "Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf).\n",
    "\n",
    "You can get the GloVe text file with the embeddings from the link above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('data/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = read_csv('data/train_emoji.csv')\n",
    "X_test, Y_test = read_csv('data/test_emoji.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to find the maximum sequence length in our training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxLen = len(max(X_train, key=lambda x: len(x.split())).split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the X values into a list of indices and convert the outputs into one hot vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_indices = sentences_to_indices(X_train, word_to_index, maxLen)\n",
    "Y_train_oh = convert_to_one_hot(Y_train, C = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding matrix maps word indices to embedding vectors. In TensorFlow Keras, the embedding matrix is represented as a \"layer.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Embedding() layer's input is an integer matrix of size **(batch size, max input length)**\n",
    "\n",
    "The embedding layer outputs an array of shape **(batch size, max input length, dimension of word vectors)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/embedding1.png\" style=\"width:700px;height:250px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement this layer by doing the following:\n",
    "\n",
    "1. Initialize the embedding matrix as a numpy array of zeros.\n",
    "    * The embedding matrix has a row for each unique word in the vocabulary.\n",
    "    * There is one additional row to handle \"unknown\" words.\n",
    "    * So vocab_size is the number of unique words plus one.\n",
    "    * Each row will store the vector representation of one word. \n",
    "        * For example, one row may be 50 positions long if using GloVe word vectors.\n",
    "    * In the code below, `emb_dim` represents the length of a word embedding.\n",
    "2. Fill in each row of the embedding matrix with the vector representation of a word\n",
    "    * Each word in `word_to_index` is a string.\n",
    "    * word_to_vec_map is a dictionary where the keys are strings and the values are the word vectors.\n",
    "3. Define the Keras embedding layer. \n",
    "    * Use [Embedding()](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding). \n",
    "    * The input dimension is equal to the vocabulary length (number of unique words plus one).\n",
    "    * The output dimension is equal to the number of positions in a word embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    Creates a Keras Embedding() layer and loads pre-trained GloVe 50-dimensional vectors.\n",
    "    \n",
    "    Arguments:\n",
    "    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.\n",
    "    word_to_index -- dictionary mapping words to their indices in the vocabulary (400,001 words)\n",
    "\n",
    "    Returns:\n",
    "    embedding_layer -- pretrained Embedding layer Keras instance\n",
    "    \"\"\"\n",
    "    \n",
    "    vocab_size = len(word_to_index) + 1  # Adding 1 to fit Keras embedding (requirement)\n",
    "    emb_dim = word_to_vec_map[next(iter(word_to_vec_map))].shape[0]  # Dimensionality of GloVe word vectors (50)\n",
    "      \n",
    "    # Step 1: Initialize the embedding matrix with zeros\n",
    "    emb_matrix = np.zeros((vocab_size, emb_dim))\n",
    "    \n",
    "    # Step 2: Fill the embedding matrix with GloVe word vectors\n",
    "    for word, idx in word_to_index.items():\n",
    "        emb_matrix[idx, :] = word_to_vec_map[word]\n",
    "\n",
    "    # Step 3: Create Keras Embedding layer\n",
    "    embedding_layer = Embedding(input_dim=vocab_size, output_dim=emb_dim, trainable=False)\n",
    "\n",
    "    # Build the embedding layer before setting the weights\n",
    "    embedding_layer.build((None,))\n",
    "    \n",
    "    # Set the weights of the embedding layer to the embedding matrix\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Emojify Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/emojifier-v2.png\" style=\"width:700px;height:400px;\"> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The model takes as input an array of sentences of shape (`m`, `max_len`, ) defined by `input_shape`. \n",
    "* The model outputs a softmax probability vector of shape (`m`, `C = 5`). \n",
    "\n",
    "* We will need to use the following Keras layers:\n",
    "    * [Input()](https://www.tensorflow.org/api_docs/python/tf/keras/Input)\n",
    "        * Set the `shape` and `dtype` parameters.\n",
    "        * The inputs are integers, so we can specify the data type as a string, `int32`.\n",
    "    * [LSTM()](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM)\n",
    "        * Set the `units` and `return_sequences` parameters.\n",
    "    * [Dropout()](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout)\n",
    "        * Set the `rate` parameter.\n",
    "    * [Dense()](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense)\n",
    "        * Set the `units`, \n",
    "        * Set the `Activation` to `softmax`.\n",
    "    * [Model()](https://www.tensorflow.org/api_docs/python/tf/keras/Model)\n",
    "        * Set `inputs` and `outputs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Emojify(input_shape, word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    Create Emojify-v2 model's graph.\n",
    "    \n",
    "    Args:\n",
    "    input_shape -- shape of the input, usually (max_len,)\n",
    "    word_to_vec_map -- dictionary mapping words to their 50-dimensional vector representations\n",
    "    word_to_index -- dictionary mapping words to their indices in the vocabulary\n",
    "\n",
    "    Returns:\n",
    "    model -- Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    # Input layer for sentence indices\n",
    "    sentence_indices = Input(shape=input_shape, dtype='int32')\n",
    "    \n",
    "    # Embedding layer\n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "    embeddings = embedding_layer(sentence_indices)\n",
    "    \n",
    "    # LSTM layer 1\n",
    "    X = LSTM(units=128, return_sequences=True)(embeddings)\n",
    "    X = Dropout(rate=0.5)(X)\n",
    "    \n",
    "    # LSTM layer 2\n",
    "    X = LSTM(units=128, return_sequences=False)(X)\n",
    "    X = Dropout(rate=0.5)(X)\n",
    "    \n",
    "    # Dense layer\n",
    "    X = Dense(units=5, activation='softmax')(X)\n",
    "    \n",
    "    # Create the model\n",
    "    model = Model(inputs=sentence_indices, outputs=X)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 10)]              0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 10, 50)            20000050  \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 10, 128)           91648     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 10, 128)           0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 128)               131584    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,223,927\n",
      "Trainable params: 223,877\n",
      "Non-trainable params: 20,000,050\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Emojify((maxLen,), word_to_vec_map, word_to_index)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "5/5 [==============================] - 6s 19ms/step - loss: 1.5796 - accuracy: 0.2273\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1.5402 - accuracy: 0.3258\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.4693 - accuracy: 0.4015\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.4507 - accuracy: 0.3864\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1.3760 - accuracy: 0.4697\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.3619 - accuracy: 0.4242\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.2329 - accuracy: 0.5076\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.1870 - accuracy: 0.5758\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 1.0273 - accuracy: 0.5909\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.9444 - accuracy: 0.6136\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.6977 - accuracy: 0.7424\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.7717 - accuracy: 0.6591\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.6667 - accuracy: 0.7652\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.7679 - accuracy: 0.6742\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 26ms/step - loss: 0.6156 - accuracy: 0.7500\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5262 - accuracy: 0.8182\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5236 - accuracy: 0.8485\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.4750 - accuracy: 0.8561\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3397 - accuracy: 0.9318\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.3832 - accuracy: 0.8561\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3452 - accuracy: 0.8409\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.3101 - accuracy: 0.8864\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.2504 - accuracy: 0.9318\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.2580 - accuracy: 0.9015\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.2420 - accuracy: 0.9394\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.2385 - accuracy: 0.9318\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.2071 - accuracy: 0.9318\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.1635 - accuracy: 0.9545\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.2200 - accuracy: 0.9318\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.1700 - accuracy: 0.9470\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.1778 - accuracy: 0.9318\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.1616 - accuracy: 0.9470\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.1709 - accuracy: 0.9545\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.1435 - accuracy: 0.9545\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0910 - accuracy: 0.9773\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.1047 - accuracy: 0.9848\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0843 - accuracy: 0.9697\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0683 - accuracy: 0.9773\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.1089 - accuracy: 0.9697\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0691 - accuracy: 0.9697\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0581 - accuracy: 0.9848\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.1088 - accuracy: 0.9545\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0483 - accuracy: 0.9924\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0820 - accuracy: 0.9697\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.0434 - accuracy: 0.9924\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0267 - accuracy: 0.9924\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0324 - accuracy: 0.9848\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.7401 - accuracy: 0.8106\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.4545 - accuracy: 0.8712\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.2974 - accuracy: 0.9015\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fe3eb89d10>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_indices, Y_train_oh, epochs = 50, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see the performance of the model on the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 15ms/step - loss: 0.9350 - accuracy: 0.7500\n",
      "\n",
      "Test accuracy =  0.75\n"
     ]
    }
   ],
   "source": [
    "X_test_indices = sentences_to_indices(X_test, word_to_index, max_len = maxLen)\n",
    "Y_test_oh = convert_to_one_hot(Y_test, C = 5)\n",
    "loss, acc = model.evaluate(X_test_indices, Y_test_oh)\n",
    "print()\n",
    "print(\"Test accuracy = \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us test the model and see how it acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_model(string):\n",
    "    \"\"\"\n",
    "    Runs the string inside the model\n",
    "    \"\"\"\n",
    "    x_test = np.array([string])\n",
    "    X_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\n",
    "    print(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 34ms/step\n",
      "I am very hungry üç¥\n"
     ]
    }
   ],
   "source": [
    "try_model('I am very hungry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n",
      "I want to play right now ‚öæ\n"
     ]
    }
   ],
   "source": [
    "try_model(\"I want to play right now\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "I love you ‚ù§Ô∏è\n"
     ]
    }
   ],
   "source": [
    "try_model(\"I love you\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
